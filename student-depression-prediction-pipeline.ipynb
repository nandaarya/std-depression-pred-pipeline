{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85eca3a6",
   "metadata": {
    "papermill": {
     "duration": 0.004848,
     "end_time": "2025-03-28T15:51:30.548248",
     "exception": false,
     "start_time": "2025-03-28T15:51:30.543400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3e5d4c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-28T15:51:30.557652Z",
     "iopub.status.busy": "2025-03-28T15:51:30.557373Z",
     "iopub.status.idle": "2025-03-28T15:53:43.074536Z",
     "shell.execute_reply": "2025-03-28T15:53:43.073446Z"
    },
    "papermill": {
     "duration": 132.52362,
     "end_time": "2025-03-28T15:53:43.076099",
     "exception": false,
     "start_time": "2025-03-28T15:51:30.552479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\r\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\r\n",
      "Collecting tfx\r\n",
      "  Downloading tfx-1.16.0-py3-none-any.whl.metadata (37 kB)\r\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.1.0)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\r\n",
      "Collecting tensorflow_model_analysis==0.46.0\r\n",
      "  Downloading tensorflow_model_analysis-0.46.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_analysis==0.46.0) (1.4.0)\r\n",
      "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_analysis==0.46.0) (7.34.0)\r\n",
      "Collecting ipywidgets<8,>=7 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading ipywidgets-7.8.5-py2.py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_analysis==0.46.0) (1.26.4)\r\n",
      "Collecting pandas<2,>=1.0 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_analysis==0.46.0) (11.0.0)\r\n",
      "Collecting pyarrow<11,>=10 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\n",
      "Collecting rouge-score<2,>=0.1.2 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting sacrebleu<4,>=2.3 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_analysis==0.46.0) (1.13.1)\r\n",
      "Requirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_analysis==0.46.0) (1.17.0)\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\r\n",
      "Collecting tensorflow-estimator>=2.10 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting tensorflow-metadata<1.16.0,>=1.15.0 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting tfx-bsl<1.16.0,>=1.15.1 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading tfx_bsl-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting apache-beam<3,>=2.47 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading apache_beam-2.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\r\n",
      "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_analysis==0.46.0) (3.20.3)\r\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.5.4)\r\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.1.0)\r\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.4.5)\r\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter) (5.5.6)\r\n",
      "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.10/dist-packages (from jupyter) (3.6.8)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\r\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\r\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\r\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\r\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\r\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\r\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\r\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting ml-pipelines-sdk==1.16.0 (from tfx)\r\n",
      "  Downloading ml_pipelines_sdk-1.16.0-py3-none-any.whl.metadata (33 kB)\r\n",
      "Collecting ml-metadata<1.17.0,>=1.16.0 (from tfx)\r\n",
      "  Downloading ml_metadata-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\r\n",
      "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.5.2)\r\n",
      "Requirement already satisfied: docker<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (7.1.0)\r\n",
      "Collecting google-apitools<1,>=0.5 (from tfx)\r\n",
      "  Downloading google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting google-api-python-client<2,>=1.8 (from tfx)\r\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.1.4)\r\n",
      "Collecting attrs<24,>=19.3.0 (from tfx)\r\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Requirement already satisfied: click<9,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (8.1.7)\r\n",
      "Requirement already satisfied: google-api-core<3 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.34.1)\r\n",
      "Requirement already satisfied: google-cloud-aiplatform<2,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.74.0)\r\n",
      "Requirement already satisfied: google-cloud-bigquery<4,>=3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.25.0)\r\n",
      "Requirement already satisfied: keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.7)\r\n",
      "Collecting kubernetes<27,>=10.0.1 (from tfx)\r\n",
      "  Downloading kubernetes-26.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: orjson!=3.10.7 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.10.12)\r\n",
      "Collecting scipy<2,>=1.4.1 (from tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scikit-learn\r\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.10/dist-packages (from tfx) (6.0.2)\r\n",
      "INFO: pip is looking at multiple versions of tfx to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting tfx\r\n",
      "  Downloading tfx-1.15.1-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting ml-pipelines-sdk==1.15.1 (from tfx)\r\n",
      "  Downloading ml_pipelines_sdk-1.15.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting ml-metadata<1.16.0,>=1.15.0 (from tfx)\r\n",
      "  Downloading ml_metadata-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\r\n",
      "Collecting docker<5,>=4.1 (from tfx)\r\n",
      "  Downloading docker-4.4.4-py2.py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting kubernetes<13,>=10.0.1 (from tfx)\r\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting tensorflow-hub<0.16,>=0.15.0 (from tfx)\r\n",
      "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting tensorflow-data-validation<1.16.0,>=1.15.1 (from tfx)\r\n",
      "  Downloading tensorflow_data_validation-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n",
      "Collecting tensorflow-serving-api<2.16,>=2.15 (from tfx)\r\n",
      "  Downloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting tensorflow-transform<1.16.0,>=1.15.0 (from tfx)\r\n",
      "  Downloading tensorflow_transform-1.15.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\r\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\r\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting cloudpickle~=2.2.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\r\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\r\n",
      "  Downloading grpcio-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.22.0)\r\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (4.23.0)\r\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\r\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading objsize-0.7.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (4.11.1)\r\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (1.25.0)\r\n",
      "Collecting pydot<2,>=1.2.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\r\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2025.1)\r\n",
      "Collecting redis<6,>=5.0.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\r\n",
      "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2024.11.6)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.32.3)\r\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.4.0)\r\n",
      "Collecting zstandard<1,>=0.18.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\n",
      "Collecting pyarrow-hotfix<1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (5.5.0)\r\n",
      "Collecting google-api-core<3 (from tfx)\r\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting google-apitools<1,>=0.5 (from tfx)\r\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.27.0)\r\n",
      "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.2.0)\r\n",
      "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.20.2)\r\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.27.1)\r\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading google_cloud_pubsublite-1.12.0-py2.py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Collecting google-cloud-storage<3,>=2.18.2 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\r\n",
      "Collecting google-cloud-bigquery-storage<3,>=2.6.3 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading google_cloud_bigquery_storage-2.30.0-py3-none-any.whl.metadata (9.8 kB)\r\n",
      "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.4.1)\r\n",
      "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.27.0)\r\n",
      "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading google_cloud_spanner-3.53.0-py2.py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading google_cloud_dlp-3.29.0-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.16.0)\r\n",
      "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.16.0)\r\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (3.10.0)\r\n",
      "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading google_cloud_recommendations_ai-0.10.17-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Collecting keyrings.google-artifactregistry-auth (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading keyrings.google_artifactregistry_auth-1.1.2-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.8.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.66.0)\r\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.8->tfx)\r\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\r\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.14.0)\r\n",
      "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.0.7)\r\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.11.0a2)\r\n",
      "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (0.16)\r\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=3->tfx) (2.7.2)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (0.19.2)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (4.4.2)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (0.7.5)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (5.7.1)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (3.0.48)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (2.19.1)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (0.2.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (0.1.7)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow_model_analysis==0.46.0) (4.9.0)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow_model_analysis==0.46.0) (0.2.2)\r\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow_model_analysis==0.46.0) (0.2.0)\r\n",
      "Collecting widgetsnbextension~=3.6.10 (from ipywidgets<8,>=7->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading widgetsnbextension-3.6.10-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets<8,>=7->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading jupyterlab_widgets-1.1.11-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=2.7.3->tfx) (3.0.2)\r\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (1.0.5)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2025.1.31)\r\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2.3.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (2.4.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker<2,>=1.3.1->tfx) (5.9.5)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score<2,>=0.1.2->tensorflow_model_analysis==0.46.0) (3.2.4)\r\n",
      "Collecting portalocker (from sacrebleu<4,>=2.3->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow_model_analysis==0.46.0) (0.9.0)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow_model_analysis==0.46.0) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow_model_analysis==0.46.0) (5.3.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\r\n",
      "Collecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx)\r\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (8.6.3)\r\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (6.3.3)\r\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (5.7.2)\r\n",
      "Requirement already satisfied: jupyterlab-server~=2.19 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.27.3)\r\n",
      "Requirement already satisfied: jupyter-server<3,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.12.5)\r\n",
      "Requirement already satisfied: jupyter-ydoc~=0.2.4 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (0.2.5)\r\n",
      "Requirement already satisfied: jupyter-server-ydoc~=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (0.8.0)\r\n",
      "Requirement already satisfied: nbclassic in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (1.1.0)\r\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.2.1)\r\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (24.0.1)\r\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (23.1.0)\r\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (5.10.4)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.6.0)\r\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.8.3)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.18.1)\r\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.21.1)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.8.4)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.3.0)\r\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.4)\r\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (6.2.0)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.5.1)\r\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.6.0)\r\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.7.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (4.12.3)\r\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.5.13)\r\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.6.2->tfx) (1.48.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (4.9)\r\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.13.1)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (1.29.0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (1.29.0)\r\n",
      "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (7.7.0)\r\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.5.3)\r\n",
      "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\r\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.18.2->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (1.6.0)\r\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0)\r\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (3.2.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow_model_analysis==0.46.0) (0.8.4)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2024.10.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.22.3)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->jupyterlab->jupyter) (4.3.6)\r\n",
      "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (3.7.1)\r\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.12.0)\r\n",
      "Requirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.5.3)\r\n",
      "Requirement already satisfied: jupyter-server-fileid<1,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (0.9.3)\r\n",
      "Requirement already satisfied: ypy-websocket<0.9.0,>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (0.8.4)\r\n",
      "Requirement already satisfied: y-py<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-ydoc~=0.2.4->jupyterlab->jupyter) (0.6.2)\r\n",
      "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server~=2.19->jupyterlab->jupyter) (2.16.0)\r\n",
      "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server~=2.19->jupyterlab->jupyter) (0.10.0)\r\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic->jupyterlab->jupyter) (0.2.4)\r\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->jupyter) (2.21.1)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.6.1)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow_model_analysis==0.46.0) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow_model_analysis==0.46.0) (0.2.13)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (2.29.0)\r\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (2.7.0)\r\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (5.0.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (3.10)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.6)\r\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\r\n",
      "Requirement already satisfied: keyring in /usr/lib/python3/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (23.5.0)\r\n",
      "Requirement already satisfied: pluggy in /usr/local/lib/python3.10/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (1.5.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (2024.2.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.3.1)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.2.2)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.23.5->tensorflow_model_analysis==0.46.0) (2024.2.0)\r\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (3.2.1)\r\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.1.4)\r\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.1.1)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (1.2.15)\r\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (8.5.0)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (0.50b0)\r\n",
      "Requirement already satisfied: aiofiles<23,>=22.1.0 in /usr/local/lib/python3.10/dist-packages (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (22.1.0)\r\n",
      "Requirement already satisfied: aiosqlite<1,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (0.21.0)\r\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.17.1)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.22)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_model_analysis==0.46.0) (3.21.0)\r\n",
      "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.5.1)\r\n",
      "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (20.11.0)\r\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (3.0.0)\r\n",
      "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.3.0)\r\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (24.11.1)\r\n",
      "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.3.0)\r\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (2.9.0.20241206)\r\n",
      "Downloading tensorflow_model_analysis-0.46.0-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\r\n",
      "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tfx-1.15.1-py3-none-any.whl (3.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ml_pipelines_sdk-1.15.1-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading apache_beam-2.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading grpcio-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ipywidgets-7.8.5-py2.py3-none-any.whl (124 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ml_metadata-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorflow_data_validation-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\r\n",
      "Downloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl (26 kB)\r\n",
      "Downloading tensorflow_transform-1.15.0-py3-none-any.whl (451 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tfx_bsl-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\r\n",
      "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\r\n",
      "Downloading google_cloud_bigquery_storage-2.30.0-py3-none-any.whl (256 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_cloud_dlp-3.29.0-py3-none-any.whl (213 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.9/213.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_cloud_pubsublite-1.12.0-py2.py3-none-any.whl (322 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_cloud_recommendations_ai-0.10.17-py3-none-any.whl (211 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.3/211.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_cloud_spanner-3.53.0-py2.py3-none-any.whl (483 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.1/483.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jupyterlab_widgets-1.1.11-py3-none-any.whl (246 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading objsize-0.7.1-py3-none-any.whl (11 kB)\r\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\n",
      "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\r\n",
      "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\r\n",
      "Downloading widgetsnbextension-3.6.10-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading keyrings.google_artifactregistry_auth-1.1.2-py3-none-any.whl (10 kB)\r\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\r\n",
      "Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\r\n",
      "Building wheels for collected packages: google-apitools, rouge-score, crcmod, dill, hdfs, pyfarmhash, docopt\r\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131014 sha256=543884c3c1b1c847b3590598479ff9fe7d913e09d62b6ece83c6a4d7197e682b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\r\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a388013806f09e66c83368cf3e9f8732ea3ed58cabf4c96927f2aaabb3d7e377\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\r\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31405 sha256=c8fa6be08652adadd52131f79c94125d818f907da4abea524be4469404f95826\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\r\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=d9e0d50105b17a79d8181907daf15d4186da78860067d8b3b70e0f397b301164\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\r\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=65d15d876cdeb3df153207e9d4c21b390c7821b09b4831f55da4ea0c0621d4c8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\r\n",
      "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=88657 sha256=4b6adb6c1e408e9bec64dcdb9e2ad5e05904e6cfdf55c7d43cf61d0aeb341c86\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\r\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6fafa782c190196d67a5d54469965dbab4d5ed4c1e0326210315a7d6d4dbb827\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\r\n",
      "Successfully built google-apitools rouge-score crcmod dill hdfs pyfarmhash docopt\r\n",
      "Installing collected packages: pyfarmhash, docopt, crcmod, zstandard, wrapt, uritemplate, tensorflow-metadata, tensorflow-estimator, redis, pydot, pyarrow-hotfix, portalocker, objsize, keras, jupyterlab-widgets, jsonpickle, grpcio, fasteners, fastavro, dill, cloudpickle, attrs, ml-metadata, hdfs, grpc-interceptor, docker, kubernetes, keyrings.google-artifactregistry-auth, google-apitools, google-api-core, google-api-python-client, ml-pipelines-sdk, google-cloud-storage, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-dlp, google-cloud-bigquery-storage, google-cloud-pubsublite, widgetsnbextension, ipywidgets, jupyter, tensorboard, pyarrow, ml-dtypes, tensorflow, apache-beam, tensorflow-serving-api, pandas, tfx-bsl, scipy, sacrebleu, rouge-score, tensorflow-transform, tensorflow_model_analysis, tensorflow-hub, tensorflow-data-validation, tfx\r\n",
      "  Attempting uninstall: wrapt\r\n",
      "    Found existing installation: wrapt 1.17.0\r\n",
      "    Uninstalling wrapt-1.17.0:\r\n",
      "      Successfully uninstalled wrapt-1.17.0\r\n",
      "  Attempting uninstall: uritemplate\r\n",
      "    Found existing installation: uritemplate 4.1.1\r\n",
      "    Uninstalling uritemplate-4.1.1:\r\n",
      "      Successfully uninstalled uritemplate-4.1.1\r\n",
      "  Attempting uninstall: tensorflow-metadata\r\n",
      "    Found existing installation: tensorflow-metadata 1.13.1\r\n",
      "    Uninstalling tensorflow-metadata-1.13.1:\r\n",
      "      Successfully uninstalled tensorflow-metadata-1.13.1\r\n",
      "  Attempting uninstall: pydot\r\n",
      "    Found existing installation: pydot 3.0.3\r\n",
      "    Uninstalling pydot-3.0.3:\r\n",
      "      Successfully uninstalled pydot-3.0.3\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.5.0\r\n",
      "    Uninstalling keras-3.5.0:\r\n",
      "      Successfully uninstalled keras-3.5.0\r\n",
      "  Attempting uninstall: jupyterlab-widgets\r\n",
      "    Found existing installation: jupyterlab_widgets 3.0.13\r\n",
      "    Uninstalling jupyterlab_widgets-3.0.13:\r\n",
      "      Successfully uninstalled jupyterlab_widgets-3.0.13\r\n",
      "  Attempting uninstall: jsonpickle\r\n",
      "    Found existing installation: jsonpickle 4.0.1\r\n",
      "    Uninstalling jsonpickle-4.0.1:\r\n",
      "      Successfully uninstalled jsonpickle-4.0.1\r\n",
      "  Attempting uninstall: grpcio\r\n",
      "    Found existing installation: grpcio 1.68.1\r\n",
      "    Uninstalling grpcio-1.68.1:\r\n",
      "      Successfully uninstalled grpcio-1.68.1\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.3.8\r\n",
      "    Uninstalling dill-0.3.8:\r\n",
      "      Successfully uninstalled dill-0.3.8\r\n",
      "  Attempting uninstall: cloudpickle\r\n",
      "    Found existing installation: cloudpickle 3.1.0\r\n",
      "    Uninstalling cloudpickle-3.1.0:\r\n",
      "      Successfully uninstalled cloudpickle-3.1.0\r\n",
      "  Attempting uninstall: attrs\r\n",
      "    Found existing installation: attrs 25.1.0\r\n",
      "    Uninstalling attrs-25.1.0:\r\n",
      "      Successfully uninstalled attrs-25.1.0\r\n",
      "  Attempting uninstall: docker\r\n",
      "    Found existing installation: docker 7.1.0\r\n",
      "    Uninstalling docker-7.1.0:\r\n",
      "      Successfully uninstalled docker-7.1.0\r\n",
      "  Attempting uninstall: google-api-core\r\n",
      "    Found existing installation: google-api-core 1.34.1\r\n",
      "    Uninstalling google-api-core-1.34.1:\r\n",
      "      Successfully uninstalled google-api-core-1.34.1\r\n",
      "  Attempting uninstall: google-api-python-client\r\n",
      "    Found existing installation: google-api-python-client 2.155.0\r\n",
      "    Uninstalling google-api-python-client-2.155.0:\r\n",
      "      Successfully uninstalled google-api-python-client-2.155.0\r\n",
      "  Attempting uninstall: google-cloud-storage\r\n",
      "    Found existing installation: google-cloud-storage 2.14.0\r\n",
      "    Uninstalling google-cloud-storage-2.14.0:\r\n",
      "      Successfully uninstalled google-cloud-storage-2.14.0\r\n",
      "  Attempting uninstall: widgetsnbextension\r\n",
      "    Found existing installation: widgetsnbextension 4.0.13\r\n",
      "    Uninstalling widgetsnbextension-4.0.13:\r\n",
      "      Successfully uninstalled widgetsnbextension-4.0.13\r\n",
      "  Attempting uninstall: ipywidgets\r\n",
      "    Found existing installation: ipywidgets 8.1.5\r\n",
      "    Uninstalling ipywidgets-8.1.5:\r\n",
      "      Successfully uninstalled ipywidgets-8.1.5\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.17.1\r\n",
      "    Uninstalling tensorboard-2.17.1:\r\n",
      "      Successfully uninstalled tensorboard-2.17.1\r\n",
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 19.0.1\r\n",
      "    Uninstalling pyarrow-19.0.1:\r\n",
      "      Successfully uninstalled pyarrow-19.0.1\r\n",
      "  Attempting uninstall: ml-dtypes\r\n",
      "    Found existing installation: ml-dtypes 0.4.1\r\n",
      "    Uninstalling ml-dtypes-0.4.1:\r\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.17.1\r\n",
      "    Uninstalling tensorflow-2.17.1:\r\n",
      "      Successfully uninstalled tensorflow-2.17.1\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.2.3\r\n",
      "    Uninstalling pandas-2.2.3:\r\n",
      "      Successfully uninstalled pandas-2.2.3\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.13.1\r\n",
      "    Uninstalling scipy-1.13.1:\r\n",
      "      Successfully uninstalled scipy-1.13.1\r\n",
      "  Attempting uninstall: tensorflow-hub\r\n",
      "    Found existing installation: tensorflow-hub 0.16.1\r\n",
      "    Uninstalling tensorflow-hub-0.16.1:\r\n",
      "      Successfully uninstalled tensorflow-hub-0.16.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.1 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\r\n",
      "cudf-cu12 25.2.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "cudf-cu12 25.2.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 10.0.1 which is incompatible.\r\n",
      "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\r\n",
      "dask-cudf-cu12 25.2.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\r\n",
      "dask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 10.0.1 which is incompatible.\r\n",
      "datasets 3.3.1 requires pyarrow>=15.0.0, but you have pyarrow 10.0.1 which is incompatible.\r\n",
      "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\r\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\r\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.24.2 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\r\n",
      "langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\r\n",
      "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\r\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\r\n",
      "plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\r\n",
      "plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "pylibcudf-cu12 25.2.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 10.0.1 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.15.1 which is incompatible.\r\n",
      "tensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.15.1 which is incompatible.\r\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\r\n",
      "visions 0.7.6 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\r\n",
      "xarray 2024.11.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed apache-beam-2.63.0 attrs-23.2.0 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 docker-4.4.4 docopt-0.6.2 fastavro-1.10.0 fasteners-0.19 google-api-core-2.24.2 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-bigquery-storage-2.30.0 google-cloud-dlp-3.29.0 google-cloud-pubsublite-1.12.0 google-cloud-recommendations-ai-0.10.17 google-cloud-spanner-3.53.0 google-cloud-storage-2.19.0 grpc-interceptor-0.15.4 grpcio-1.65.5 hdfs-2.7.3 ipywidgets-7.8.5 jsonpickle-3.4.2 jupyter-1.1.1 jupyterlab-widgets-1.1.11 keras-2.15.0 keyrings.google-artifactregistry-auth-1.1.2 kubernetes-12.0.1 ml-dtypes-0.3.2 ml-metadata-1.15.0 ml-pipelines-sdk-1.15.1 objsize-0.7.1 pandas-1.5.3 portalocker-3.1.1 pyarrow-10.0.1 pyarrow-hotfix-0.6 pydot-1.4.2 pyfarmhash-0.3.2 redis-5.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 scipy-1.12.0 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-data-validation-1.15.1 tensorflow-estimator-2.15.0 tensorflow-hub-0.15.0 tensorflow-metadata-1.15.0 tensorflow-serving-api-2.15.1 tensorflow-transform-1.15.0 tensorflow_model_analysis-0.46.0 tfx-1.15.1 tfx-bsl-1.15.1 uritemplate-3.0.1 widgetsnbextension-3.6.10 wrapt-1.14.1 zstandard-0.23.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter scikit-learn tensorflow tfx flask joblib tensorflow_model_analysis==0.46.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a906eeb",
   "metadata": {
    "papermill": {
     "duration": 0.020667,
     "end_time": "2025-03-28T15:53:43.117814",
     "exception": false,
     "start_time": "2025-03-28T15:53:43.097147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24fa8dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:53:43.160619Z",
     "iopub.status.busy": "2025-03-28T15:53:43.160348Z",
     "iopub.status.idle": "2025-03-28T15:54:16.206897Z",
     "shell.execute_reply": "2025-03-28T15:54:16.206209Z"
    },
    "papermill": {
     "duration": 33.070285,
     "end_time": "2025-03-28T15:54:16.208367",
     "exception": false,
     "start_time": "2025-03-28T15:53:43.138082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Text\n",
    " \n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    " \n",
    "PIPELINE_NAME = \"nandaaryaputra-pipeline\"\n",
    " \n",
    "# pipeline inputs\n",
    "DATA_ROOT = \"/kaggle/input/student-depression-dataset\"\n",
    "COMPONENTS_FILE = \"modules/components.py\"\n",
    "PIPELINE_FILE = \"modules/pipeline.py\"\n",
    "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
    "TUNER_MODULE_FILE = \"modules/tuner.py\"\n",
    "TRAINER_MODULE_FILE = \"modules/trainer.py\"\n",
    " \n",
    "# pipeline outputs\n",
    "OUTPUT_BASE = \"output\"\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04cc14b",
   "metadata": {
    "papermill": {
     "duration": 0.020882,
     "end_time": "2025-03-28T15:54:16.250356",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.229474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create modules package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34104e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:16.291394Z",
     "iopub.status.busy": "2025-03-28T15:54:16.291108Z",
     "iopub.status.idle": "2025-03-28T15:54:16.437614Z",
     "shell.execute_reply": "2025-03-28T15:54:16.436494Z"
    },
    "papermill": {
     "duration": 0.168482,
     "end_time": "2025-03-28T15:54:16.438886",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.270404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfc0ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:16.482320Z",
     "iopub.status.busy": "2025-03-28T15:54:16.481966Z",
     "iopub.status.idle": "2025-03-28T15:54:16.622250Z",
     "shell.execute_reply": "2025-03-28T15:54:16.621296Z"
    },
    "papermill": {
     "duration": 0.16294,
     "end_time": "2025-03-28T15:54:16.623743",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.460803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!touch modules/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb32bb",
   "metadata": {
    "papermill": {
     "duration": 0.019696,
     "end_time": "2025-03-28T15:54:16.663716",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.644020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate component.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee12138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:16.705455Z",
     "iopub.status.busy": "2025-03-28T15:54:16.705158Z",
     "iopub.status.idle": "2025-03-28T15:54:16.712156Z",
     "shell.execute_reply": "2025-03-28T15:54:16.711398Z"
    },
    "papermill": {
     "duration": 0.029397,
     "end_time": "2025-03-28T15:54:16.713358",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.683961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {COMPONENTS_FILE}\n",
    "\"\"\"\n",
    "This module initializes the TFX pipeline components.\n",
    "\n",
    "The components include data ingestion, data validation, feature transformation, \n",
    "hyperparameter tuning, model training, model evaluation, and model deployment. \n",
    "It builds a pipeline using TensorFlow Extended (TFX) components and integrates \n",
    "them for end-to-end machine learning workflows.\n",
    "\n",
    "Main Components:\n",
    "- CsvExampleGen: Ingests data from CSV files.\n",
    "- StatisticsGen: Computes dataset statistics.\n",
    "- SchemaGen: Infers the schema of the dataset.\n",
    "- ExampleValidator: Detects anomalies and missing values.\n",
    "- Transform: Applies feature engineering transformations.\n",
    "- Tuner: Optimizes hyperparameters using a tuning module.\n",
    "- Trainer: Trains a machine learning model.\n",
    "- Resolver: Fetches the latest blessed model for evaluation comparison.\n",
    "- Evaluator: Analyzes model performance using predefined metrics.\n",
    "- Pusher: Deploys the trained model if it passes evaluation.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "from tfx.components import (\n",
    "    CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator,\n",
    "    Transform, Tuner, Trainer, Evaluator, Pusher\n",
    ")\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
    "    LatestBlessedModelStrategy\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ComponentConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for initializing TFX components.\n",
    "\n",
    "    Attributes:\n",
    "        data_dir (str): Directory where the input data is stored.\n",
    "        transform_module (str): Path to the transformation module.\n",
    "        tuner_module (str): Path to the hyperparameter tuning module.\n",
    "        training_module (str): Path to the model training module.\n",
    "        training_steps (int): Number of steps for training.\n",
    "        eval_steps (int): Number of steps for evaluation.\n",
    "        serving_model_dir (str): Directory to store the trained model for deployment.\n",
    "    \"\"\"\n",
    "    data_dir: str\n",
    "    transform_module: str\n",
    "    tuner_module: str\n",
    "    training_module: str\n",
    "    training_steps: int\n",
    "    eval_steps: int\n",
    "    serving_model_dir: str\n",
    "\n",
    "def init_components(config: ComponentConfig):\n",
    "    \"\"\"\n",
    "    Initializes TFX components required for building a pipeline for training and deploying a model.\n",
    "    \"\"\"\n",
    "    # Disable pylint no-member as some attributes are dynamically generated \n",
    "    # and not recognized by pylint. Source: https://stackoverflow.com/a/54156144\n",
    "    \n",
    "    # pylint: disable=no-member\n",
    "    output_config = example_gen_pb2.Output(\n",
    "        split_config=example_gen_pb2.SplitConfig(\n",
    "            splits=[\n",
    "                example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
    "                example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)\n",
    "            ]\n",
    "        )\n",
    "    ) \n",
    "    # pylint: enable=no-member\n",
    "    \n",
    "    example_gen = CsvExampleGen(input_base=config.data_dir, output_config=output_config)\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "    schema_gen = SchemaGen(statistics=statistics_gen.outputs[\"statistics\"])\n",
    "    example_validator = ExampleValidator(statistics=statistics_gen.outputs['statistics'], schema=schema_gen.outputs['schema'])\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'], \n",
    "        schema=schema_gen.outputs['schema'], \n",
    "        module_file=os.path.abspath(config.transform_module)\n",
    "    )\n",
    "\n",
    "    # pylint: disable=no-member\n",
    "    tuner = Tuner(\n",
    "        module_file=os.path.abspath(config.tuner_module),\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        train_args=trainer_pb2.TrainArgs(splits=[\"train\"], num_steps=config.training_steps),\n",
    "        eval_args=trainer_pb2.EvalArgs(splits=[\"eval\"], num_steps=config.eval_steps),\n",
    "    )\n",
    "    # pylint: enable=no-member\n",
    "\n",
    "    # pylint: disable=no-member\n",
    "    trainer = Trainer(\n",
    "        module_file=os.path.abspath(config.training_module),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        hyperparameters=tuner.outputs[\"best_hyperparameters\"],\n",
    "        train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=config.training_steps),\n",
    "        eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=config.eval_steps)\n",
    "    )\n",
    "    # pylint: enable=no-member\n",
    "    \n",
    "    model_resolver = Resolver(\n",
    "        strategy_class=LatestBlessedModelStrategy,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(type=ModelBlessing)\n",
    "    ).with_id('Latest_blessed_model_resolver')\n",
    "    \n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='Depression')],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                tfma.MetricConfig(class_name='AUC'),\n",
    "                tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                tfma.MetricConfig(class_name='TruePositives'),\n",
    "                tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value': 0.5}\n",
    "                        ),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value': 0.0001}\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config\n",
    "    )\n",
    "\n",
    "    # pylint: disable=no-member\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        model_blessing=evaluator.outputs[\"blessing\"],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=config.serving_model_dir\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    # pylint: enable=no-member\n",
    "    \n",
    "    return (\n",
    "        example_gen, statistics_gen, schema_gen, example_validator, \n",
    "        transform, tuner, trainer, model_resolver, evaluator, pusher\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158be01b",
   "metadata": {
    "papermill": {
     "duration": 0.019815,
     "end_time": "2025-03-28T15:54:16.753214",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.733399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate pipeline.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcff7e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:16.794117Z",
     "iopub.status.busy": "2025-03-28T15:54:16.793897Z",
     "iopub.status.idle": "2025-03-28T15:54:16.798815Z",
     "shell.execute_reply": "2025-03-28T15:54:16.797942Z"
    },
    "papermill": {
     "duration": 0.026663,
     "end_time": "2025-03-28T15:54:16.800112",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.773449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PIPELINE_FILE}\n",
    "\"\"\"\n",
    "Pipeline definition for the student depression prediction model.\n",
    "\n",
    "This script initializes and runs a TFX pipeline for data transformation,\n",
    "hyperparameter tuning, model training, and evaluation.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Text\n",
    "\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from modules.components import ComponentConfig, init_components\n",
    "\n",
    "PIPELINE_NAME = \"nandaaryaputra-pipeline\"\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/student-depression-dataset\"\n",
    "COMPONENTS_FILE = \"modules/components.py\"\n",
    "PIPELINE_FILE = \"modules/pipeline.py\"\n",
    "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
    "TUNER_MODULE_FILE = \"modules/tuner.py\"\n",
    "TRAINER_MODULE_FILE = \"modules/trainer.py\"\n",
    "\n",
    "OUTPUT_BASE = \"output\"\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")\n",
    "\n",
    "def init_local_pipeline(\n",
    "    components_list, root_dir: Text\n",
    ") -> pipeline.Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize a local TFX pipeline.\n",
    "\n",
    "    Args:\n",
    "        components_list: A list of TFX components to be included in the pipeline.\n",
    "        root_dir: Root directory for pipeline output artifacts.\n",
    "\n",
    "    Returns:\n",
    "        A TFX pipeline.\n",
    "    \"\"\"\n",
    "    logging.info(f'Pipeline root set to: {root_dir}')\n",
    "    beam_args = [\n",
    "        '--direct_running_mode=multi_processing',\n",
    "        '--direct_num_workers=0'\n",
    "    ]\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=root_dir,\n",
    "        components=components_list,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path\n",
    "        ),\n",
    "        beam_pipeline_args=beam_args\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "\n",
    "    config = ComponentConfig(\n",
    "        data_dir=DATA_ROOT,\n",
    "        training_module=TRAINER_MODULE_FILE,\n",
    "        transform_module=TRANSFORM_MODULE_FILE,\n",
    "        tuner_module=TUNER_MODULE_FILE,\n",
    "        training_steps=800,\n",
    "        eval_steps=400,\n",
    "        serving_model_dir=serving_model_dir\n",
    "    )\n",
    "\n",
    "    pipeline_comps = init_components(config)\n",
    "\n",
    "    student_pipeline = init_local_pipeline(pipeline_comps, pipeline_root)\n",
    "    BeamDagRunner().run(pipeline=student_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ae026",
   "metadata": {
    "papermill": {
     "duration": 0.01982,
     "end_time": "2025-03-28T15:54:16.839818",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.819998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate transform.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ffc9fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:16.881495Z",
     "iopub.status.busy": "2025-03-28T15:54:16.881260Z",
     "iopub.status.idle": "2025-03-28T15:54:16.885898Z",
     "shell.execute_reply": "2025-03-28T15:54:16.885163Z"
    },
    "papermill": {
     "duration": 0.027039,
     "end_time": "2025-03-28T15:54:16.887102",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.860063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "\"\"\"\n",
    "This module defines the feature preprocessing function for the TFX pipeline.\n",
    "\n",
    "It applies transformations to the input dataset using TensorFlow Transform (TFT).\n",
    "The transformations include feature scaling, categorical encoding, and data cleaning.\n",
    "Unused features are removed to optimize model performance.\n",
    "\n",
    "Main Transformations:\n",
    "- Standardization: Numeric features are normalized using Z-score normalization.\n",
    "- One-Hot Encoding: Categorical features are encoded with vocabulary-based one-hot encoding.\n",
    "- Data Cleaning: Entries with missing values in \"Financial Stress\" are removed.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "LABEL_KEY = \"Depression\"\n",
    "NUMERIC_FEATURES = [\"Academic Pressure\", \"Age\", \"CGPA\", \"Study Satisfaction\", \"Work/Study Hours\"]\n",
    "CATEGORICAL_FEATURES = [\"Dietary Habits\", \"Family History of Mental Illness\", \n",
    "                        \"Financial Stress\", \"Gender\", \"Have you ever had suicidal thoughts ?\", \n",
    "                        \"Sleep Duration\"]\n",
    "\n",
    "UNUSED_FEATURES = [\"Job Satisfaction\", \"Work Pressure\", \"id\", \"City\", \"Profession\", \"Degree\"]\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"\n",
    "    Converts feature names by replacing spaces and special characters \n",
    "    to make them compatible with TensorFlow Transform.\n",
    "    \"\"\"\n",
    "    return key.replace(\"/\", \"_\").replace(\" \", \"_\").lower() + \"_xf\"\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Applies preprocessing to input features.\n",
    "\n",
    "    Args:\n",
    "        inputs: A dictionary mapping feature keys to raw tensors.\n",
    "\n",
    "    Returns:\n",
    "        outputs: A dictionary mapping feature keys to transformed tensors.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    # Remove unused features\n",
    "    filtered_inputs = {key: tensor for key, tensor in inputs.items() if key not in UNUSED_FEATURES}\n",
    "\n",
    "    # Remove data where 'Financial Stress' is missing (\"?\")\n",
    "    mask = tf.not_equal(filtered_inputs[\"Financial Stress\"], \"?\")\n",
    "    clean_inputs = {key: tf.boolean_mask(tensor, mask) for key, tensor in filtered_inputs.items()}\n",
    "\n",
    "    # Standardize numerical features using Z-score normalization\n",
    "    for feature in NUMERIC_FEATURES:\n",
    "        outputs[transformed_name(feature)] = tft.scale_to_z_score(clean_inputs[feature])\n",
    "\n",
    "    # One-Hot Encoding for categorical features\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        vocab_filename = feature.replace(\" \", \"_\").lower() + \"_vocab\"\n",
    "\n",
    "        # Apply encoding using the created vocabulary\n",
    "        indexed = tft.compute_and_apply_vocabulary(\n",
    "            clean_inputs[feature],\n",
    "            vocab_filename=vocab_filename,\n",
    "            num_oov_buckets=1\n",
    "        )\n",
    "\n",
    "        vocab_size = tf.cast(tft.experimental.get_vocabulary_size_by_name(vocab_filename) + 1, tf.int32)\n",
    "\n",
    "        # One-Hot Encoding\n",
    "        one_hot_encoded = tf.one_hot(indexed, depth=vocab_size, on_value=1.0, off_value=0.0)\n",
    "\n",
    "        outputs[transformed_name(feature)] = one_hot_encoded\n",
    "\n",
    "    # Retain the target label without modification\n",
    "    outputs[LABEL_KEY] = clean_inputs[LABEL_KEY]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49431ef3",
   "metadata": {
    "papermill": {
     "duration": 0.022366,
     "end_time": "2025-03-28T15:54:16.929707",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.907341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate tuner.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297dd16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:16.976360Z",
     "iopub.status.busy": "2025-03-28T15:54:16.976009Z",
     "iopub.status.idle": "2025-03-28T15:54:16.982377Z",
     "shell.execute_reply": "2025-03-28T15:54:16.981354Z"
    },
    "papermill": {
     "duration": 0.032009,
     "end_time": "2025-03-28T15:54:16.983726",
     "exception": false,
     "start_time": "2025-03-28T15:54:16.951717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TUNER_MODULE_FILE}\n",
    "# pylint: disable=no-member\n",
    "\"\"\"\n",
    "This module defines the hyperparameter tuning process for a deep learning model \n",
    "using Keras Tuner and TensorFlow Transform (TFT) in a TFX pipeline.\n",
    "\n",
    "The tuner applies Bayesian Optimization to search for the best hyperparameters \n",
    "for a binary classification model predicting student depression.\n",
    "\n",
    "Main Components:\n",
    "- `input_fn()`: Reads and prepares the dataset for training and evaluation.\n",
    "- `model_builder()`: Defines the neural network architecture with tunable parameters.\n",
    "- `tuner_fn()`: Configures and executes the Bayesian Optimization tuning process.\n",
    "\"\"\"\n",
    "\n",
    "from typing import NamedTuple, Dict, Text, Any\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import tensorflow_transform as tft\n",
    "from keras_tuner.engine import base_tuner\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from modules.transform import transformed_name, LABEL_KEY, NUMERIC_FEATURES, CATEGORICAL_FEATURES\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "TunerFnResult = NamedTuple(\"TunerFnResult\", [\n",
    "    (\"tuner\", base_tuner.BaseTuner),\n",
    "    (\"fit_kwargs\", Dict[Text, Any]),\n",
    "])\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"\n",
    "    Reads GZIP-compressed TFRecord files and returns a dataset.\n",
    "    \"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training and evaluation.\n",
    "    \"\"\"\n",
    "    feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key=LABEL_KEY)\n",
    "    return dataset\n",
    "\n",
    "def build_inputs(tf_transform_output):\n",
    "    \"\"\"\n",
    "    Creates input layers for numeric and categorical features.\n",
    "    Returns a dictionary of inputs and concatenated feature tensors.\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    \n",
    "    # Numeric Features\n",
    "    numeric_inputs = [tf.keras.Input(shape=(1,), name=transformed_name(f), dtype=tf.float32) for f in NUMERIC_FEATURES]\n",
    "    for f in NUMERIC_FEATURES:\n",
    "        inputs[transformed_name(f)] = numeric_inputs[NUMERIC_FEATURES.index(f)]\n",
    "    \n",
    "    concat_numeric = layers.concatenate(numeric_inputs)\n",
    "    \n",
    "    # Categorical Features\n",
    "    categorical_inputs = []\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        transformed_feature_name = transformed_name(feature)\n",
    "        vocab_size = tf_transform_output.vocabulary_size_by_name(feature.replace(\" \", \"_\").lower() + \"_vocab\") + 1\n",
    "        \n",
    "        categorical_input = tf.keras.Input(shape=(vocab_size,), name=transformed_feature_name, dtype=tf.float32)\n",
    "        categorical_inputs.append(categorical_input)\n",
    "        inputs[transformed_feature_name] = categorical_input\n",
    "    \n",
    "    concat_categorical = layers.concatenate(categorical_inputs)\n",
    "\n",
    "    return inputs, concat_numeric, concat_categorical\n",
    "\n",
    "def model_builder(hp, tf_transform_output):\n",
    "    \"\"\"\n",
    "    Builds a Keras model with tunable hyperparameters.\n",
    "    \"\"\"\n",
    "    inputs, concat_numeric, concat_categorical = build_inputs(tf_transform_output)\n",
    "    \n",
    "    x = layers.Dense(hp.Int(\"dense_units_1\", min_value=32, max_value=128, step=32), activation='relu')(concat_numeric)\n",
    "    combined = layers.concatenate([x, concat_categorical])\n",
    "    x = layers.Dense(hp.Int(\"dense_units_2\", min_value=32, max_value=128, step=32), activation='relu')(combined)\n",
    "    x = layers.Dropout(hp.Float(\"dropout_rate\", min_value=0.1, max_value=0.5, step=0.1))(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs):\n",
    "    \"\"\"\n",
    "    Defines the hyperparameter tuning function using Bayesian Optimization.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, NUM_EPOCHS)\n",
    "    eval_set = input_fn(fn_args.eval_files, tf_transform_output, NUM_EPOCHS)\n",
    "    \n",
    "    tuner = kt.BayesianOptimization(\n",
    "        hypermodel=lambda hp: model_builder(hp, tf_transform_output),\n",
    "        objective=kt.Objective('binary_accuracy', direction='max'),\n",
    "        max_trials=30,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name=\"bayesian_tuning\",\n",
    "    )\n",
    "    \n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            \"x\": train_set,\n",
    "            \"validation_data\": eval_set,\n",
    "            \"epochs\": NUM_EPOCHS,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1417e",
   "metadata": {
    "papermill": {
     "duration": 0.023398,
     "end_time": "2025-03-28T15:54:17.030926",
     "exception": false,
     "start_time": "2025-03-28T15:54:17.007528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate trainer.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "572ae4a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:17.079252Z",
     "iopub.status.busy": "2025-03-28T15:54:17.078907Z",
     "iopub.status.idle": "2025-03-28T15:54:17.084884Z",
     "shell.execute_reply": "2025-03-28T15:54:17.083783Z"
    },
    "papermill": {
     "duration": 0.031956,
     "end_time": "2025-03-28T15:54:17.086339",
     "exception": false,
     "start_time": "2025-03-28T15:54:17.054383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "# pylint: disable=no-member\n",
    "\"\"\"\n",
    "This module defines the training pipeline for a deep learning model used in a TFX pipeline.\n",
    "\n",
    "The model predicts depression risk based on various numerical and categorical features.\n",
    "It utilizes TensorFlow Transform (TFT) for preprocessing and TensorFlow Keras for training.\n",
    "\n",
    "Main Components:\n",
    "- `input_fn()`: Reads and prepares the dataset for training and validation.\n",
    "- `model_builder()`: Defines the neural network architecture.\n",
    "- `_get_serve_tf_example_fn()`: Creates a serving function for model inference.\n",
    "- `run_fn()`: Trains the model and exports it for serving.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras import layers\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from modules.transform import LABEL_KEY\n",
    "from modules.tuner import input_fn, build_inputs\n",
    "\n",
    "def model_builder(tf_transform_output):\n",
    "    \"\"\"\n",
    "    Build the deep learning model using transformed features and best hyperparameter from tuning.\n",
    "    \"\"\"\n",
    "    inputs, concat_numeric, concat_categorical = build_inputs(tf_transform_output)\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(concat_numeric)\n",
    "    combined = layers.concatenate([x, concat_categorical])\n",
    "    x = layers.Dense(64, activation='relu')(combined)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def _get_serve_tf_example_fn(model, tf_transform_output):\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        \n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        \n",
    "        return model(transformed_features)\n",
    "    \n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def run_fn(fn_args: FnArgs):\n",
    "    \"\"\"Train and save the model.\"\"\"\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', patience=10)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(fn_args.serving_model_dir, \"best_model.keras\"),\n",
    "        monitor='val_binary_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    # Load TFT transform graph\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    \n",
    "    # Load training and validation datasets\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, num_epochs=10)\n",
    "    val_set = input_fn(fn_args.eval_files, tf_transform_output, num_epochs=10)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = model_builder(tf_transform_output)\n",
    "    model.fit(\n",
    "        x=train_set,\n",
    "        validation_data=val_set,\n",
    "        callbacks=[tensorboard_callback, es, mc],\n",
    "        epochs=10)\n",
    "\n",
    "    signatures = {\n",
    "    \"serving_default\": _get_serve_tf_example_fn(\n",
    "        model, tf_transform_output\n",
    "    ).get_concrete_function(\n",
    "        tf.TensorSpec(\n",
    "            shape=[None], \n",
    "            dtype=tf.string, \n",
    "            name=\"examples\"\n",
    "        )\n",
    "    )}\n",
    "    \n",
    "    # Save final model\n",
    "    tf.saved_model.save(model, fn_args.serving_model_dir, signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2dc423",
   "metadata": {
    "papermill": {
     "duration": 0.020775,
     "end_time": "2025-03-28T15:54:37.053016",
     "exception": false,
     "start_time": "2025-03-28T15:54:37.032241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "859ab270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:54:37.096077Z",
     "iopub.status.busy": "2025-03-28T15:54:37.095783Z",
     "iopub.status.idle": "2025-03-28T17:16:01.467388Z",
     "shell.execute_reply": "2025-03-28T17:16:01.466617Z"
    },
    "papermill": {
     "duration": 4884.394877,
     "end_time": "2025-03-28T17:16:01.468854",
     "exception": false,
     "start_time": "2025-03-28T15:54:37.073977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 02m 26s]\n",
      "binary_accuracy: 0.8821601271629333\n",
      "\n",
      "Best binary_accuracy So Far: 0.8836020231246948\n",
      "Total elapsed time: 01h 13m 53s\n",
      "Results summary\n",
      "Results in output/nandaaryaputra-pipeline/Tuner/.system/executor_execution/7/.temp/7/bayesian_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"binary_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8836020231246948\n",
      "\n",
      "Trial 20 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.883534848690033\n",
      "\n",
      "Trial 23 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8821780681610107\n",
      "\n",
      "Trial 29 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8821601271629333\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8820481896400452\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8816003799438477\n",
      "\n",
      "Trial 25 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8811973929405212\n",
      "\n",
      "Trial 27 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8808033466339111\n",
      "\n",
      "Trial 28 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 128\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8806645274162292\n",
      "\n",
      "Trial 14 summary\n",
      "Hyperparameters:\n",
      "dense_units_1: 96\n",
      "dense_units_2: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "Score: 0.8801450729370117\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " academic_pressure_xf (Inpu  [(None, 1)]                  0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " age_xf (InputLayer)         [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " cgpa_xf (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " study_satisfaction_xf (Inp  [(None, 1)]                  0         []                            \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " work_study_hours_xf (Input  [(None, 1)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 5)                    0         ['academic_pressure_xf[0][0]',\n",
      " )                                                                   'age_xf[0][0]',              \n",
      "                                                                     'cgpa_xf[0][0]',             \n",
      "                                                                     'study_satisfaction_xf[0][0]'\n",
      "                                                                    , 'work_study_hours_xf[0][0]']\n",
      "                                                                                                  \n",
      " dietary_habits_xf (InputLa  [(None, 5)]                  0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " family_history_of_mental_i  [(None, 3)]                  0         []                            \n",
      " llness_xf (InputLayer)                                                                           \n",
      "                                                                                                  \n",
      " financial_stress_xf (Input  [(None, 6)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " gender_xf (InputLayer)      [(None, 3)]                  0         []                            \n",
      "                                                                                                  \n",
      " have_you_ever_had_suicidal  [(None, 3)]                  0         []                            \n",
      " _thoughts_?_xf (InputLayer                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sleep_duration_xf (InputLa  [(None, 6)]                  0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 64)                   384       ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 26)                   0         ['dietary_habits_xf[0][0]',   \n",
      " )                                                                   'family_history_of_mental_ill\n",
      "                                                                    ness_xf[0][0]',               \n",
      "                                                                     'financial_stress_xf[0][0]', \n",
      "                                                                     'gender_xf[0][0]',           \n",
      "                                                                     'have_you_ever_had_suicidal_t\n",
      "                                                                    houghts_?_xf[0][0]',          \n",
      "                                                                     'sleep_duration_xf[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 90)                   0         ['dense_3[0][0]',             \n",
      " )                                                                   'concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 64)                   5824      ['concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 32)                   2080      ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 1)                    33        ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8321 (32.50 KB)\n",
      "Trainable params: 8321 (32.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "3490/3490 [==============================] - 24s 6ms/step - loss: 0.3538 - binary_accuracy: 0.8461 - val_loss: 0.3594 - val_binary_accuracy: 0.8417\n",
      "Epoch 2/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.3426 - binary_accuracy: 0.8492 - val_loss: 0.3666 - val_binary_accuracy: 0.8446\n",
      "Epoch 3/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.3331 - binary_accuracy: 0.8524 - val_loss: 0.3778 - val_binary_accuracy: 0.8387\n",
      "Epoch 4/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.3236 - binary_accuracy: 0.8571 - val_loss: 0.4023 - val_binary_accuracy: 0.8372\n",
      "Epoch 5/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.3147 - binary_accuracy: 0.8605 - val_loss: 0.4140 - val_binary_accuracy: 0.8345\n",
      "Epoch 6/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.3066 - binary_accuracy: 0.8643 - val_loss: 0.4428 - val_binary_accuracy: 0.8363\n",
      "Epoch 7/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.3012 - binary_accuracy: 0.8655 - val_loss: 0.4440 - val_binary_accuracy: 0.8367\n",
      "Epoch 8/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.2961 - binary_accuracy: 0.8676 - val_loss: 0.4570 - val_binary_accuracy: 0.8291\n",
      "Epoch 9/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.2909 - binary_accuracy: 0.8694 - val_loss: 0.5101 - val_binary_accuracy: 0.8320\n",
      "Epoch 10/10\n",
      "3490/3490 [==============================] - 22s 6ms/step - loss: 0.2869 - binary_accuracy: 0.8709 - val_loss: 0.5029 - val_binary_accuracy: 0.8354\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from modules.pipeline import init_local_pipeline\n",
    "from modules.components import init_components, ComponentConfig\n",
    "\n",
    "PIPELINE_NAME = \"nandaaryaputra-pipeline\"\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/student-depression-dataset\"\n",
    "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
    "TUNER_MODULE_FILE = \"modules/tuner.py\"\n",
    "TRAINER_MODULE_FILE = \"modules/trainer.py\"\n",
    "\n",
    "OUTPUT_BASE = \"output\"\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")\n",
    "\n",
    "config = ComponentConfig(\n",
    "    data_dir=DATA_ROOT,\n",
    "    transform_module=TRANSFORM_MODULE_FILE,\n",
    "    tuner_module=TUNER_MODULE_FILE,\n",
    "    training_module=TRAINER_MODULE_FILE,\n",
    "    training_steps=800,\n",
    "    eval_steps=400,\n",
    "    serving_model_dir=serving_model_dir\n",
    ")\n",
    "\n",
    "components = init_components(config)\n",
    "\n",
    "pipeline = init_local_pipeline(components, pipeline_root)\n",
    "BeamDagRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e63c6",
   "metadata": {
    "papermill": {
     "duration": 0.192768,
     "end_time": "2025-03-28T17:16:01.861859",
     "exception": false,
     "start_time": "2025-03-28T17:16:01.669091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check code score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b441c948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:16:02.298316Z",
     "iopub.status.busy": "2025-03-28T17:16:02.297992Z",
     "iopub.status.idle": "2025-03-28T17:16:06.608492Z",
     "shell.execute_reply": "2025-03-28T17:16:06.607290Z"
    },
    "papermill": {
     "duration": 4.504865,
     "end_time": "2025-03-28T17:16:06.610664",
     "exception": false,
     "start_time": "2025-03-28T17:16:02.105799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autopep8 in /usr/local/lib/python3.10/dist-packages (2.3.2)\r\n",
      "Requirement already satisfied: pylint in /usr/local/lib/python3.10/dist-packages (3.3.6)\r\n",
      "Requirement already satisfied: pycodestyle>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from autopep8) (2.12.1)\r\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8) (2.2.1)\r\n",
      "Requirement already satisfied: astroid<=3.4.0.dev0,>=3.3.8 in /usr/local/lib/python3.10/dist-packages (from pylint) (3.3.9)\r\n",
      "Requirement already satisfied: dill>=0.2 in /usr/local/lib/python3.10/dist-packages (from pylint) (0.3.1.1)\r\n",
      "Requirement already satisfied: isort!=5.13,<7,>=4.2.5 in /usr/local/lib/python3.10/dist-packages (from pylint) (6.0.1)\r\n",
      "Requirement already satisfied: mccabe<0.8,>=0.6 in /usr/local/lib/python3.10/dist-packages (from pylint) (0.7.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.2 in /usr/local/lib/python3.10/dist-packages (from pylint) (4.3.6)\r\n",
      "Requirement already satisfied: tomlkit>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from pylint) (0.13.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from astroid<=3.4.0.dev0,>=3.3.8->pylint) (4.12.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install autopep8 pylint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a58eab19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:16:07.005876Z",
     "iopub.status.busy": "2025-03-28T17:16:07.005553Z",
     "iopub.status.idle": "2025-03-28T17:16:22.122068Z",
     "shell.execute_reply": "2025-03-28T17:16:22.120895Z"
    },
    "papermill": {
     "duration": 15.314343,
     "end_time": "2025-03-28T17:16:22.124123",
     "exception": false,
     "start_time": "2025-03-28T17:16:06.809780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "--------------------------------------------------------------------\r\n",
      "Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!autopep8 --in-place --aggressive --aggressive modules/components.py modules/pipeline.py modules/transform.py modules/trainer.py modules/tuner.py\n",
    "!pylint modules/components.py modules/pipeline.py modules/transform.py modules/trainer.py modules/tuner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcff222",
   "metadata": {
    "papermill": {
     "duration": 0.192011,
     "end_time": "2025-03-28T17:16:22.508575",
     "exception": false,
     "start_time": "2025-03-28T17:16:22.316564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf06da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T17:16:22.911817Z",
     "iopub.status.busy": "2025-03-28T17:16:22.911475Z",
     "iopub.status.idle": "2025-03-28T17:16:27.249790Z",
     "shell.execute_reply": "2025-03-28T17:16:27.248703Z"
    },
    "papermill": {
     "duration": 4.55212,
     "end_time": "2025-03-28T17:16:27.251546",
     "exception": false,
     "start_time": "2025-03-28T17:16:22.699426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip freeze >> requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6857137,
     "sourceId": 11013442,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5103.015674,
   "end_time": "2025-03-28T17:16:30.875630",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-28T15:51:27.859956",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
